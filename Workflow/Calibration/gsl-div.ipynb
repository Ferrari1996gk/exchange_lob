{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sc\n",
    "from functools import reduce\n",
    "\n",
    "def get_symbolised_ts_old(ts, b, L, min_per=1, max_per=99, state_space=None):\n",
    "    \"\"\"Symbolise a time-series based on sensitivity b.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts: np.array\n",
    "        array of all time series to be symbolised,\n",
    "        first entry is the original data\n",
    "    b: int\n",
    "        the amount of symbols to be used for the\n",
    "        time-series representation\n",
    "    L: int\n",
    "        the number of symbol word lengths to be checked\n",
    "        (trade of individual correlation or patterns)\n",
    "    min_per: int\n",
    "        percentile to be used as minimum cut-off\n",
    "    max_per: int\n",
    "        percentile to be used as maximum cut-off\n",
    "    state_space: tuple\n",
    "        state space can be given when the boundaries of the\n",
    "        state space are known and not taken from the time-series data\n",
    "    Return\n",
    "    ------\n",
    "    a list of dataframes that contain symbol timeseries for\n",
    "    combinations of b and L\n",
    "    Notes\n",
    "    -----\n",
    "    min_per and max_per are used here instead of min and max\n",
    "    values of the data to avoid sensitivity to outliers.\n",
    "    \"\"\"\n",
    "    # if no state space is defined we generate our own based on\n",
    "    # either the standard percentiles or those given by the user\n",
    "    cuts = []\n",
    "    if not state_space:\n",
    "        for x in ts:\n",
    "            min_p = np.percentile(x, min_per)\n",
    "            max_p = np.percentile(x, max_per)\n",
    "            cuts.append(np.linspace(min_p, max_p, b+1))\n",
    "    else:\n",
    "        cuts = np.linspace(state_space[0], state_space[1], b+1)\n",
    "    # now we map the time series to the bins in the symbol space\n",
    "    symbolised_ts = np.array([np.clip(np.digitize(t, cut, right=True), 1, b) for t, cut in zip(ts,cuts)])\n",
    "    # to be able to deal with \"words\" or combination of symbols it is easier\n",
    "    # to deal with them as strings in pandas dfs\n",
    "    # TODO: Maybe better way of doing this\n",
    "    sym_str = pd.DataFrame(symbolised_ts).astype(str)\n",
    "    # collect all symbol dataframes based on block size given\n",
    "    all_dfs = []\n",
    "    all_dfs.append(sym_str)\n",
    "    for l in range(1, L):\n",
    "        df = pd.DataFrame()\n",
    "        for x in range(int(len(sym_str.columns))-l):\n",
    "            df = pd.concat([df, sym_str.loc[:, x:x+l].apply(\"\".join, axis=1)],\n",
    "                           axis=1)\n",
    "        all_dfs.append(df)\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "def get_symbolised_ts(ts, b, L, min_per=1, max_per=99, state_space=None):\n",
    "    \"\"\"Symbolise a time-series based on sensitivity b.\n",
    "    Parameters\n",
    "    ----------\n",
    "    ts: np.array\n",
    "        array of all time series to be symbolised,\n",
    "        first entry is the original data\n",
    "    b: int\n",
    "        the amount of symbols to be used for the\n",
    "        time-series representation\n",
    "    L: int\n",
    "        the number of symbol word lengths to be checked\n",
    "        (trade of individual correlation or patterns)\n",
    "    min_per: int\n",
    "        percentile to be used as minimum cut-off\n",
    "    max_per: int\n",
    "        percentile to be used as maximum cut-off\n",
    "    state_space: tuple\n",
    "        state space can be given when the boundaries of the\n",
    "        state space are known and not taken from the time-series data\n",
    "    Return\n",
    "    ------\n",
    "    a list of dataframes that contain symbol timeseries for\n",
    "    combinations of b and L\n",
    "    Notes\n",
    "    -----\n",
    "    min_per and max_per are used here instead of min and max\n",
    "    values of the data to avoid sensitivity to outliers.\n",
    "    \"\"\"\n",
    "    # if no state space is defined we generate our own based on\n",
    "    # either the standard percentiles or those given by the user\n",
    "    cuts = []\n",
    "    if not state_space:\n",
    "        for x in ts:\n",
    "            min_p = np.percentile(x, min_per)\n",
    "            max_p = np.percentile(x, max_per)\n",
    "            cuts.append(np.linspace(min_p, max_p, b+1))\n",
    "    else:\n",
    "        cuts = np.linspace(state_space[0], state_space[1], b+1)\n",
    "    # now we map the time series to the bins in the symbol space\n",
    "    symbolised_ts = np.array([np.clip(np.digitize(t, cut, right=True), 1, b) for t, cut in zip(ts,cuts)])\n",
    "    # to be able to deal with \"words\" or combination of symbols it is easier\n",
    "    # to deal with them as strings in pandas dfs\n",
    "    # TODO: Maybe better way of doing this\n",
    "    sym_str = pd.DataFrame(symbolised_ts).astype(str)\n",
    "    # collect all symbol dataframes based on block size given\n",
    "    all_dfs = []\n",
    "    all_dfs.append(sym_str)\n",
    "    tmp = sym_str.values\n",
    "    for l in range(1, L):\n",
    "        tmp = tmp[:, :-1] + sym_str.values[:, l:]\n",
    "        all_dfs.append(pd.DataFrame(tmp))\n",
    "    return all_dfs\n",
    "\n",
    "\n",
    "def get_weights(weight_type, L):\n",
    "    \"\"\"\n",
    "    Generate the weights.\n",
    "    Parameters\n",
    "    ----------\n",
    "    weight_type: str ('uniform' or 'add-progressive')\n",
    "        string defining which weighting to be used\n",
    "    L: int\n",
    "        the number of symbol word lengths to be checked\n",
    "        (trade of individual correlation or patterns)\n",
    "    \"\"\"\n",
    "    if weight_type == 'uniform':\n",
    "        w = np.array([1. / L] * L)\n",
    "    elif weight_type == 'add-progressive':\n",
    "        w = np.array([2. / (L * (L + 1))]*L).cumsum()\n",
    "    return w\n",
    "\n",
    "\n",
    "def gsl_div(original, model, weights='add-progressive',\n",
    "            b=5, L=6, min_per=1, max_per=99, state_space=None):\n",
    "    \"\"\"Calculate the gsl_div between model and reference data.\n",
    "    Parameters\n",
    "    ----------\n",
    "    original: np.array, shape(1, len(time-series))\n",
    "        original reference data\n",
    "    model: np.array, shape(len(reps), len(time-series))\n",
    "        array of model results (replicates)\n",
    "    weights: str ('add-progressive', 'uniform', )\n",
    "        Specify the weighting to be applied to different block\n",
    "        lengths.\n",
    "    b: int\n",
    "        the amount of symbols to be used for representation\n",
    "    L: int\n",
    "        the number of symbol combination to be checked\n",
    "    min_per: int\n",
    "        percentile to be used as minimum cut-off\n",
    "    max_per: int\n",
    "        percentile to be used as maximum cut-off\n",
    "    state_space: tuple\n",
    "        state space can be given when the boundaries of the\n",
    "        state space are known and not taken from the ts data\n",
    "    Notes\n",
    "    -----\n",
    "    Implementation of the following paper:\n",
    "    http://dx.doi.org/10.1016/j.ecosta.2017.01.006\n",
    "    \"\"\"\n",
    "    all_ts = np.concatenate([original, model])\n",
    "    # determine the time series length\n",
    "    T = original.shape[1]\n",
    "    if T < L:\n",
    "        raise ValueError('Word length cant be longer than timeseries')\n",
    "    # symbolise time-series\n",
    "    sym_ts = get_symbolised_ts(all_ts, b=b, L=L, min_per=min_per,\n",
    "                               max_per=max_per, state_space=state_space)\n",
    "    raw_divergence = []\n",
    "    correction = []\n",
    "    # run over all word sizes\n",
    "    for n, ts in enumerate(sym_ts):\n",
    "        # get frequency distributions for original and replicates\n",
    "        # could do it by applying a pd.value_counts to every column but this\n",
    "        # way you get a 10x speed up\n",
    "        fs = pd.DataFrame((ts.stack().reset_index()\n",
    "                           .groupby([0, \"level_0\"]).count() /\n",
    "                           len(ts.T)).unstack().values.astype(float))\n",
    "\n",
    "        # replac Calculate AIC information criterion for model selection\n",
    "        fs = fs.replace(np.nan, 0)\n",
    "        # determine the size of vocabulary for the right base in the log\n",
    "        base = b**(n+1)\n",
    "        # calculate the distances between the different time-series\n",
    "        # give a particluar word size\n",
    "        M = (fs.iloc[:, 1:].values +\n",
    "             np.expand_dims(fs.iloc[:, 0].values, 1)) / 2.\n",
    "        temp = (2 * sc.entropy(M, base=base) -\n",
    "                sc.entropy(fs.values[:, 1:], base=base))\n",
    "        raw_divergence.append(reduce(np.add, temp) /\n",
    "                              float((len(fs.columns) - 1)))\n",
    "        cardinality_of_m = fs.apply(lambda x: reduce(np.logical_or, x),\n",
    "                                    axis=1).sum()\n",
    "        # if there is only one replicate this has to be handled differently\n",
    "        if len(fs.columns) == 2:\n",
    "            cardinality_of_reps = fs.iloc[:, 1].apply(lambda x: x != 0).sum()\n",
    "        else:\n",
    "            cardinality_of_reps = fs.iloc[:, 1:].apply(\n",
    "                lambda x: reduce(np.logical_or, x), axis=1).sum()\n",
    "        # calculate correction based on formula 9 line 2 in paper\n",
    "        correction.append(2*((cardinality_of_m - 1) / (4. * T)) -\n",
    "                          ((cardinality_of_reps - 1) / (2. * T)))\n",
    "\n",
    "    w = get_weights(weight_type=weights, L=L)\n",
    "    weighted_res = (w * np.array(raw_divergence)).sum(axis=0)\n",
    "    weighted_correction = (w * np.array(correction)).sum()\n",
    "\n",
    "    return weighted_res + weighted_correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aic_mt1(P, V, alpha, kappa, beta, sigma_N, gamma=50.):\n",
    "    assert len(P) == len(V)\n",
    "    dV = V - P\n",
    "    dP = np.zeros(len(P))\n",
    "    dP[1:] = np.diff(P)\n",
    "    m = np.zeros(len(P))\n",
    "    for t in np.arange(1, len(P)):\n",
    "        dp = P[t] - P[t-1]\n",
    "        m[t] = (1. - alpha) * m[t-1] + alpha * dp if t > 1 else dp\n",
    "    mu = kappa * dV[1:-1] + beta * np.tanh(gamma * m[1:-1])\n",
    "    observed = dP[2:]\n",
    "    part1 = len(mu) * np.log(2*np.pi*np.square(sigma_N))\n",
    "    part2 = np.sum(np.square(observed - mu)) / np.square(sigma_N)\n",
    "    part3 = 2. * 3\n",
    "    return part1 + part2 + part3\n",
    "\n",
    "def aic_mt2(P, V, alpha_hf, alpha_lf, kappa, beta_hf, beta_lf, sigma_N, gamma=50.):\n",
    "    assert len(P) == len(V)\n",
    "    dV = V - P\n",
    "    dP = np.zeros(len(P))\n",
    "    dP[1:] = np.diff(P)\n",
    "    mhf = np.zeros(len(P))\n",
    "    mlf = np.zeros(len(P))\n",
    "    for t in np.arange(1, len(P)):\n",
    "        dp = P[t] - P[t-1]\n",
    "        mhf[t] = (1. - alpha_hf) * mhf[t-1] + alpha_hf * dp if t > 1 else dp\n",
    "        mlf[t] = (1. - alpha_lf) * mlf[t-1] + alpha_lf * dp if t > 1 else dp\n",
    "    mu = kappa * dV[1:-1] + beta_hf * np.tanh(gamma * mhf[1:-1]) + beta_lf * np.tanh(gamma * mlf[1:-1])\n",
    "    observed = dP[2:]\n",
    "    part1 = len(mu) * np.log(2*np.pi*np.square(sigma_N))\n",
    "    part2 = np.sum(np.square(observed - mu)) / np.square(sigma_N)\n",
    "    part3 = 2. * 4\n",
    "    return part1 + part2 + part3e NaN with 0 so that log does not complain later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 99)\n",
      "(2, 99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5373207261135469"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_ts = pd.DataFrame(np.arange(1,100)).values.T\n",
    "model_ts = pd.DataFrame(np.random.normal(0,1,99)).values.T\n",
    "print(model_ts.shape)\n",
    "res = gsl_div(original_ts, model_ts, 'add-progressive',b=5, L=10, min_per=0, max_per=100, state_space=None)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.335579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5211587350913142"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = datetime.now()\n",
    "# z = pd.DataFrame(np.arange(1,50.1, 0.5)).values.T\n",
    "z = np.random.normal(0, 1, (10, 99))\n",
    "res = gsl_div(original_ts, z, 'add-progressive',b=5, L=10, min_per=0, max_per=100, state_space=None)\n",
    "print(datetime.now()-t1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = pd.DataFrame([-1,-1,1,1,1,-1]).values.T\n",
    "model = pd.DataFrame([1,1,1,-1,-1,1]).values.T\n",
    "ts = np.concatenate([original, model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[   0  1  2  3  4  5\n",
       " 0  1  1  2  2  2  1\n",
       " 1  2  2  2  1  1  2,\n",
       "     0   0   0   0   0\n",
       " 0  11  12  22  22  21\n",
       " 1  22  22  21  11  12,\n",
       "      0    0    0    0\n",
       " 0  112  122  222  221\n",
       " 1  222  221  211  112,\n",
       "       0     0     0\n",
       " 0  1122  1222  2221\n",
       " 1  2221  2211  2112,\n",
       "        0      0\n",
       " 0  11222  12221\n",
       " 1  22211  22112]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_symbolised_ts_old(ts, b=2, L=5, min_per=0, max_per=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = original_ts\n",
    "model = z\n",
    "all_ts = np.concatenate([original, model])\n",
    "b=3; L=3; min_per=0; max_per=100\n",
    "ts = all_ts\n",
    "cuts = []\n",
    "for x in ts:\n",
    "    min_p = np.percentile(x, min_per)\n",
    "    max_p = np.percentile(x, max_per)\n",
    "    cuts.append(np.linspace(min_p, max_p, b+1))\n",
    "symbolised_ts = np.array([np.clip(np.digitize(t, cut, right=True), 1, b) for t, cut in zip(ts,cuts)])\n",
    "\n",
    "print(original, model)\n",
    "print('----------')\n",
    "print(ts)\n",
    "print('----------')\n",
    "print(min_p, max_p, cuts)\n",
    "print('----------')\n",
    "print(symbolised_ts)\n",
    "print('----------')\n",
    "all_dfs = get_symbolised_ts(ts, b, L, min_per=0, max_per=100, state_space=None)\n",
    "for i in all_dfs:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
